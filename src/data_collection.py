import pandas as pd
import os

print("ğŸš€ data_collection.py started")

RAW_DATA_PATH = "data/exoplanet_data.csv"
PROCESSED_DATA_PATH = "data/exoplanet_data_cleaned.csv"


def load_dataset(path):
    print(f"ğŸ“‚ Trying to load dataset from: {path}")

    if not os.path.exists(path):
        print("âŒ File does NOT exist!")
        return None

    try:
        df = pd.read_csv(
            path,
            engine="python",
            on_bad_lines="skip"
        )
        print("âœ… Dataset loaded successfully")
        print(f"ğŸ“ Dataset shape: {df.shape}")
        return df
    except Exception as e:
        print("âŒ Error loading dataset:", e)
        return None


def validate_dataset(df):
    print("\nğŸ“Š Dataset Info:")
    print(df.info())

    print("\nğŸ” Missing Values (Top 20):")
    print(df.isnull().sum().head(20))

    print("\nğŸ“ˆ Statistical Summary:")
    print(df.describe(include="all").head())


def clean_dataset(df):
    print("\nğŸ§¹ Cleaning dataset...")

    before = df.shape[0]
    df = df.drop_duplicates()
    after = df.shape[0]

    print(f"ğŸ—‘ Removed {before - after} duplicate rows")

    return df


def save_dataset(df, path):
    df.to_csv(path, index=False)
    print(f"\nğŸ’¾ Cleaned dataset saved to: {path}")


def main():
    print("ğŸ”„ Starting data collection pipeline...")

    df = load_dataset(RAW_DATA_PATH)
    if df is None:
        print("âŒ Data loading failed. Exiting.")
        return

    validate_dataset(df)
    df_cleaned = clean_dataset(df)
    save_dataset(df_cleaned, PROCESSED_DATA_PATH)

    print("\nğŸ‰ Data collection completed successfully!")


if __name__ == "__main__":
    main()
